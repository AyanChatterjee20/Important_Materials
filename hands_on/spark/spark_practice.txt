from pyspark.sql import Row
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
from pyspark.sql.functions import *

data = [(1, "John", 30, "Sales", 50000.0),
(2, "Alice", 28, "Marketing", 60000.0),
(3, "Bob", 32, "Finance", 55000.0),
(4, "Sarah", 29, "Sales", 52000.0),
(5, "Mike", 31, "Finance", 58000.0)
]

df = spark.createDataFrame(data, schema=['id' , 'name' , 'age' , 'department' , 'salary' ])

--------------------------------------------------------------------------------------
Calculate the average salary for each department

avg_df=df.groupBy(col("department")).agg(avg("salary").alias("avg_salary"))
-------------------------------------------------------------------------------------
Add a new column named "bonus" that is 10% of the salary for all

df=df.selectExpr("*", "0.1*salary as bonus")
----------------------------------------------------------------------------------------
Group the data by department and find the employee with
the highest salary in each department

from pyspark.sql.window import Window

w= Window.partitionBy(col("department")).orderBy(col("salary").desc())
df_w=df.withColumn("rank",rank().over(w))
df_w.filter(col("rank")==1).show()
-----------------------------------------------------------------------------------------------
Find the top 2 departments with the highest total salary.

df_d=df.groupBy(col("department")).agg(sum(col("salary")).alias("sum_sal_dept")).orderBy(desc(col("sum_sal_dept"))).limit(2)
df_d.show()
----------------------------------------------------------------------------------------------
Filter the DataFrame to keep only employees aged 30 or above and working in the "Sales" department

df_s=df.filter((col("age")>=30) & (col("department")==lit("Sales")))
df_s.show()
---------------------------------------------------------------------------------------------
Calculate the difference between each employee's salary and the average salary of their respective department

w=Window.partitionBy(col("department"))
df_a=df.withColumn("avg_dept_sal",avg(col("salary")).over(w))
df_a=df_a.withColumn("diff_avg_sal",abs(col("avg_dept_sal")-col("salary")))
df_a.show()
--------------------------------------------------------------------------------------------------
Calculate the sum of salaries for employees whose names start with the letter "J".

df_j=df.filter(col("name").like("J%")).agg(sum(col("salary")).alias("total_sal"))
df_j.show()
-----------------------------------------------------------------------------------------------------------
Sort the DataFrame based on the "age" column in ascending order and then by "salary" column in descending order

df= df.orderBy(col("age"), desc(col("salary")))
df.show()
----------------------------------------------------------------------------------------------------------
Replace the department name "Finance" with "Financial Services" in the DataFrame:

df=df.withColumn("department",when(col("department")==lit("Finance"),lit("Financial Services")).otherwise(col("department")))
df.show()
-----------------------------------------------------------------------------
Calculate the percentage of total salary each employee contributes to their respective department.

w=Window.partitionBy(col("department"))
df=df.withColumn("per_sal",sum(col("salary")).over(w)).withColumn("per_sal",round((col("salary")/col("per_sal"))*100,2))
df.show()
----------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------
calculate the month-wise cumulative revenue

data = [(3000, '22-may'),
(5000, '23-may'),
(5000, '25-may'),
(10000, '22-june'),
(12500, '03-july')
]

df = spark.createDataFrame(data, schema=['revenue','date'])
df=df.withColumn("month",split(col("date"),'-')[1]).withColumn("date",date_format(to_date(col("date"),'dd-MMMM'),'dd-MM'))
w=Window.partitionBy(col("month")).orderBy(col("date"))
df=df.withColumn("cum_sum",sum(col("revenue")).over(w))
df.show()
---------------------------------------------------------------------------------------------------------
word count

data=[['I am parcticing Spark'], ['I am going to change company']]
df=spark.createDataFrame(data,['sen'])
df=df.withColumn("words",explode(split(col("sen"),' ')))
df=df.groupBy(col("words")).agg(count(col("words")).alias("word_count"))
df.show()
----------------------------------------------------------------------------------------------------------
