from pyspark.sql import Row
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
from pyspark.sql.functions import *

data = [(1, "John", 30, "Sales", 50000.0),
(2, "Alice", 28, "Marketing", 60000.0),
(3, "Bob", 32, "Finance", 55000.0),
(4, "Sarah", 29, "Sales", 52000.0),
(5, "Mike", 31, "Finance", 58000.0)
]

df = spark.createDataFrame(data, schema=['id' , 'name' , 'age' , 'department' , 'salary' ])

--------------------------------------------------------------------------------------
Calculate the average salary for each department

avg_df=df.groupBy(col("department")).agg(avg("salary").alias("avg_salary"))
-------------------------------------------------------------------------------------
Add a new column named "bonus" that is 10% of the salary for all

df=df.selectExpr("*", "0.1*salary as bonus")
----------------------------------------------------------------------------------------
Group the data by department and find the employee with
the highest salary in each department

from pyspark.sql.window import Window

w= Window.partitionBy(col("department")).orderBy(col("salary").desc())
df_w=df.withColumn("rank",rank().over(w))
df_w.filter(col("rank")==1).show()
-----------------------------------------------------------------------------------------------
Find the top 2 departments with the highest total salary.

df_d=df.groupBy(col("department")).agg(sum(col("salary")).alias("sum_sal_dept")).orderBy(desc(col("sum_sal_dept"))).limit(2)
df_d.show()
----------------------------------------------------------------------------------------------
Filter the DataFrame to keep only employees aged 30 or above and working in the "Sales" department

df_s=df.filter((col("age")>=lit(30)) & (col("department")==lit("Sales")))
df_s.show()
---------------------------------------------------------------------------------------------
Calculate the difference between each employee's salary and the average salary of their respective department

w=Window.partitionBy(col("department"))
df_a=df.withColumn("avg_dept_sal",avg(col("salary")).over(w))
df_a=df_a.withColumn("diff_avg_sal",abs(col("avg_dept_sal")-col("salary")))
df_a.show()
--------------------------------------------------------------------------------------------------
Calculate the sum of salaries for employees whose names start with the letter "J".

df_j=df.filter(col("name").like("J%")).agg(sum(col("salary")).alias("total_sal"))
df_j.show()
-----------------------------------------------------------------------------------------------------------
Sort the DataFrame based on the "age" column in ascending order and then by "salary" column in descending order

df= df.orderBy(col("age"), desc(col("salary")))
df.show()
----------------------------------------------------------------------------------------------------------
Replace the department name "Finance" with "Financial Services" in the DataFrame:

df=df.withColumn("department",when(col("department")==lit("Finance"),lit("Financial Services")).otherwise(col("department")))
df.show()
-----------------------------------------------------------------------------
Calculate the percentage of total salary each employee contributes to their respective department.

w=Window.partitionBy(col("department"))
df=df.withColumn("per_sal",sum(col("salary")).over(w)).withColumn("per_sal",round((col("salary")/col("per_sal"))*100,2))
df.show()
----------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------
calculate the month-wise cumulative revenue

data = [(3000, '22-may'),
(5000, '23-may'),
(5000, '25-may'),
(10000, '22-june'),
(12500, '03-july')
]

df = spark.createDataFrame(data, schema=['revenue','date'])
df=df.withColumn("month",split(col("date"),'-')[1]).withColumn("date",date_format(to_date(col("date"),'dd-MMMM'),'dd-MM'))
w=Window.partitionBy(col("month")).orderBy(col("date"))
df=df.withColumn("cum_sum",sum(col("revenue")).over(w))
df.show()
---------------------------------------------------------------------------------------------------------
word count

data=[['I am parcticing Spark'], ['I am going to change company']]
df=spark.createDataFrame(data,['sen'])
df=df.withColumn("words",explode(split(col("sen"),' ')))
df=df.groupBy(col("words")).agg(count(col("words")).alias("word_count"))
df.show()
----------------------------------------------------------------------------------------------------------
arrayArrayData = [("Abhishek", [["Java", "scala", "perl"], ["spark","java"]]), ("Nitya", [["spark", "java", "c++"], ["spark", "java"]]), ("Sandeep", [["csharp", "vb"], ["spark", "python"]])]
df = spark.createDataFrame(data = arrayArrayData, schema = ['name', 'subjects'])

df.printSchema()
df.show(truncate=False)
from pyspark.sql.functions import explode, flatten

df.select(df.name, explode(df.subjects)).show(truncate=False)
df.select(df.name, flatten(df.subjects)).show(truncate=False)

df=df.select(df.name, flatten(df.subjects).alias("flatten"))
df.select(df.name, explode(df.flatten)).show(truncate=False)
----------------------------------------------------------------------------------------------------------------
df = spark.createDataFrame([["1", "2019-07-01 12:01:19.000"], ["2", "2019-06-24 12:01:19.000"]], ["id", "input_timestamp"])

df.printSchema()
df.show()

df = df.withColumn("timestamptype", to_timestamp("input_timestamp"))
df = df.select(col("id"),"timestamptype","input_timestamp", to_date(col("input_timestamp")).alias("date"))
df.show()
----------------------------------------------------------------------------------------------------------------
sampleData = (("Ram", "Sales", 3000), ("Meena", "Sales", 4600), ("Abhishek", "Sales", 4100), ("Kunal", "Finance", 3000), ("Ram", "Sales", 3000), ("Srishti", "Management", 3300), ("Sandeep", "Finance", 3900), ("Hitesh", "Marketing", 3000), ("Kailash", "Marketing", 2000), ("Shyam", "Sales", 4100) )
columns = ["Employee_Name", "Department", "Salary"]
df = spark.createDataFrame(data=sampleData,schema=columns)

df.printSchema()
df.show()

windowPartitionAgg = Window.partitionBy("Department")
w = Window.partitionBy("Department").orderBy("salary").rangeBetween(Window.currentRow, Window.unboundedFollowing)
df=df.withColumn("Avg",avg(col("salary")).over(windowPartitionAgg)).withColumn("Sum",sum(col("salary")).over(windowPartitionAgg)).withColumn("Min",min(col("salary")).over(windowPartitionAgg)).withColumn("Max",max(col("salary")).over(windowPartitionAgg)).withColumn("cum_sum",sum("salary").over(w))
df.show()


sampleData = ((101, "Ram", "Biology", 80), (103, "Sita", "Social Science", 78), (104, "Lakshman", "Sanskrit", 58), (102, "Kunal", "Phisycs", 89), (101, "Ram", "Biology", 80), (106, "Srishti", "Maths", 70), (108, "Sandeep", "Physics", 75), (107, "Hitesh", "Maths", 88), (109, "Kailash", "Maths", 90), (105, "Abhishek", "Social Science", 84) )
columns = ["Roll_No", "Student_Name", "Subject", "Marks"]
df2 = spark.createDataFrame(data=sampleData, schema=columns)
windowPartition = Window.partitionBy("Subject").orderBy("Marks")
df2.printSchema()
df2.show()


sampleData = ((101, "Ram", "Biology", 80), (103, "Sita", "Social Science", 78), (104, "Lakshman", "Sanskrit", 58), (102, "Kunal", "Physics", 89), (101, "Ram", "Biology", 80), (106, "Srishti", "Maths", 70), (108, "Sandeep", "Physics", 75), (107, "Hitesh", "Maths", 88), (109, "Kailash", "Maths", 90), (105, "Abhishek", "Social Science", 84) )
columns = ["Roll_No", "Student_Name", "Subject", "Marks"]
df = spark.createDataFrame(data=sampleData, schema=columns)

windowPartition = Window.partitionBy("Subject").orderBy("Marks")
df.printSchema()
df.show()

df=df.withColumn("row_number", row_number().over(windowPartition)).withColumn("rank", rank().over(windowPartition)).withColumn("dense_rank", dense_rank().over(windowPartition)).withColumn("lead", lead("Marks",1).over(windowPartition)).withColumn("lag", lag("Marks",1).over(windowPartition))
df.printSchema()
df.show()
-------------------------------------------------------------------------------------------------------------------------------------

