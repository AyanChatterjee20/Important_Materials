cMap = {"k1" : "v1", "k2" : "v1", "k3" : "v2", "k4" : "v2"}
a_cMap = [(k,)+(v,) for k,v in cMap.items()] 
data = spark.createDataFrame(a_cMap, ['key','val'])
data.show()
from pyspark.sql.functions import count
data = data.groupBy('key').pivot('val').agg(count('val'))
data.show()
data = data.na.fill(0)
data.show()
--------------------------------------------------------------------------------------------------------
spark.sql("select substr(date_trunc('month',current_date()),1,10) as date").show()
----------------------------------------------------------------------------------------------------------
Given a DataFrame containing product information with amounts and countries, the task is to create a concise summary table where each row represents a unique product, and the columns show the amounts associated with that product in different countries. If a product is not available in a specific country, the value in that cell should be null.

Problem:
------------------------------
| Product | Amount | Country |
|---------|--------|---------|
| Banana | 1000  | USA   |
| Carrots | 1500  | USA   |
| Banana | 400  | China  |

Output:
---------------------------
| product | China | USA  |
|---------|-------|--------|
| Banana | 400  | 1000  |
| Carrots | null | 1500  |

data=[['Banana',1000,'USA'],['Carrots',1500,'USA'],['Banana',400,'China']]
df=spark.createDataFrame(data,['product','amount','country'])

df=df.groupBy(col("product")).pivot("country").agg(sum(col("amount")))
df.show()
----------------------------------------------------------------------------------------------------------------------
Given two input files:
- employee.csv with columns: employee_id, department, salary
- employee_personal.csv with columns: employee_id, first_name, last_name, DOB, state, country

Write PySpark transformations to create `employee_fact` with columns: employee_id, employee_full_name, department, salary, Salary_Diff_to_reach_highest_sal, DOB, state, country, age

df_e = spark.read.format("csv").option("header", "true").load("\\Users\\AYAN\\Desktop\\sample_input_files\\employee.csv")
df_p = spark.read.format("csv").option("header", "true").load("\\Users\\AYAN\\Desktop\\sample_input_files\\employee_personal.csv")
from pyspark.sql.functions import *
from pyspark.sql.window import Window

df=df_p.join(df_e,'employee_id',how='inner')
df=df.withColumn("employee_full_name",concat(col("first_name"),lit(" "),col("last_name"))).drop("first_name","last_name")
w=Window.partitionBy(col("department")).orderBy(col("salary")).rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)
df=df.withColumn("salary_diff_to_reach_highest_sal",max(col("salary")).over(w)-col("salary"))
df.show()

-----------------------------------------------------------------------------------------------------------------------------------
revenue date 
3000   22-may 
5000   23-may 
5000   25-may 
10000  22-june 
12500   03-july 

How would you calculate the month-wise cumulative revenue using PySpark?

data=[[3000,'22-may'],[5000,'23-may'],[5000,'25-may'],[10000,'22-june'],[12500,'03-july']]
df=spark.createDataFrame(data,['revenue','date'])

df=df.withColumn("month",split(col("date"),'-')[1]).withColumn("date",date_format(to_date(col("date"),"dd-MMMM"),"dd-MM"))

w=Window.partitionBy("month").orderBy("date")
df=df.withColumn("cum_sum",sum("revenue").over(w))
df.show()
----------------------------------------------------------------------------------------------------------------------------------
Name~|Age
Brayan,gomez~|25
John,Cleark~|30
Sumit,Sen~|31

read and show multi-delimeter sample.csv file 

df = spark.read.format("text").load("\\Users\\AYAN\\Desktop\\sample_input_files\\sample.csv")
header=df.first()[0]
schema=header.split("~|")
df=df.filter(col("value")!=header)
df=df.rdd.map(lambda x: x[0].split("~|")).toDF(schema)
df.show()
------------------------------------------------------------------------------------------------------------------------------
Write a pyspark to compute the scores of all teams after all matches. The code should return the `team_id`, `team_name`, and `num_points` for each team. Sort the results by `num_points` in descending order, and in case of a tie, order by `team_id` in ascending order.

ðŸ‘€ **Example Output:**

Teams Table:
| team_id | team_name  |
|---------|-------------|
| 10   | Leetcode FC |
| 20   | NewYork FC |
| 30   | Atlanta FC |
| 40   | Chicago FC |
| 50   | Toronto FC |

Matches Table:
| match_id | host_team | guest_team | host_goals | guest_goals |
|----------|-----------|------------|------------|-------------|
| 1    | 10    | 20     | 3     | 0      |
| 2    | 30    | 10     | 2     | 2      |
| 3    | 10    | 50     | 5     | 1      |
| 4    | 20    | 30     | 1     | 0      |
| 5    | 50    | 30     | 1     | 0      |

Result Table:
| team_id | team_name  | num_points |
|---------|-------------|------------|
| 10   | Leetcode FC | 7     |
| 20   | NewYork FC | 3     |
| 50   | Toronto FC | 3     |
| 30   | Atlanta FC | 1     |
| 40   | Chicago FC | 0     |


t=spark.createDataFrame([[10,'Leetcode FC'],[20,'NewYork FC'],[30,'Atlanta FC'],[40,'Chicago FC'],[50,'Toronto FC']],schema=['team_id','team_name'])
m=spark.createDataFrame([[1,10,20,3,0],[2,30,10,2,2],[3,10,50,5,1],[4,20,30,1,0],[5,50,30,1,0]],schema=['match_id','host_name','guest_team','host_goals','guest_goals'])

from pyspark.sql.functions import *
m1=m.withColumn("host_points",expr("""case when host_goals>guest_goals then 3 when host_goals=guest_goals then 1 else 0 end as host_points"""))
m2=m1.withColumn("guest_points",expr("""case when host_goals<guest_goals then 3 when host_goals=guest_goals then 1 else 0 end as guest_points"""))
m3=m2.groupBy(col("host_name").alias("team_id")).agg(sum(col("host_points")))
m4=m2.groupBy(col("guest_team").alias("team_id")).agg(sum(col("guest_points")))
m5=m4.join(m3,['team_id'],"inner")
m5=m5.withColumn("num_points",col("sum(guest_points)")+col("sum(host_points)")).select("team_id","num_points")
df=t.join(m5,["team_id"],"left")
df.show()
------------------------------------------------------------------------------------------------------------------------------------------
Table: `product` (Primary Key: product_id)
 | product_id | product_name |
 |------------|--------------|
 | 2     | rice     |

- Table: `product_price` (Key: product_id, effective_date)
 | product_id | price  | effective_date |
 |------------|---------|----------------|
 | 2     | 100/kg | 01/01/2023   |
 | 2     | 110/kg | 01/02/2023   |
 | 2     | 120/kg | 15/04/2023   |

- Table: `order` (Key: order_id)
 | order_id | product_id | date_sold | unit_sold |
 |----------|------------|------------|-----------|
 | 1    | 2     | 15/02/2023 | 5 kg   |

Write a pyspark code to find out the product name and total value for `order_id = 1`.

p=spark.createDataFrame([[2,'rice']],schema=['product_id','product_name'])
pp=spark.createDataFrame([[2,'100/kg','2023-01-01'],[2,'110/kg','2023-02-01'],[2,'120/kg','2023-04-15']],schema=['product_id','price','effective_date'])
o=spark.createDataFrame([[1,2,'2023-02-15','5 kg']],schema=['order_id','product_id','date_sold','unit_sold'])

from pyspark.sql.types import IntegerType
from pypsark.sql.functions import *

pp1=pp.withColumn("price",split(col("price"),'/')[0])
pp1=pp1.withColumn("price",col("price").cast(IntegerType()))

o1=o.withColumn("unit_sold",split(col("unit_sold"),' ')[0]).withColumn("unit_sold",col("unit_sold").cast(IntegerType()))

pp2=pp1.withColumn("effective_date",to_date(col("effective_date"),'yyyy-mm-dd'))
o2=o1.withColumn("date_sold",to_date(col("date_sold"),'yyyy-mm-dd'))
df1=pp2.join(o2,['product_id'],'inner')
df=df1.join(p,['product_id'],'inner')
df2=df1.filter((col("date_sold")>col("effective_date")) & (col("order_id")==1)).orderBy(col("effective_date").desc()).limit(1)
df3=df2.select("product_name","total_sum")
df3.show()
--------------------------------------------------------------------------------------------------------------------------
Task: Given a dataset with columns PERSON, TYPE, and AGE, create an output where the oldest adult is paired with the youngest child, producing pairs of ADULT and CHILD while ensuring appropriate data matching.

ðŸ’¡ Check out the input and output in the table below!

Input:--->

| PERSON | TYPE | AGE |
| ------ | ------ | --- |
| A1 | ADULT | 54 |
| A2 | ADULT | 53 |
| A3 | ADULT | 52 |
| A4 | ADULT | 58 |
| A5 | ADULT | 54 |
| C1 | CHILD | 20 |
| C2 | CHILD | 19 |
| C3 | CHILD | 22 |
| C4 | CHILD | 15 |


Expected Output:--->

| ADULT | CHILD |
| ----- | ----- |
| A4 | C4 |
| A5 | C2 |
| A1 | C1 |
| A2 | C3 |
| A3 | NULL |

df=spark.createDataFrame([['A1','ADULT',54],['A2','ADULT',53],['A3','ADULT',52],['A4','ADULT',58],['A5','ADULT',54],['C1','CHILD',20],['C2','CHILD',19],['C3','CHILD',22],['C4','CHILD',15]],schema=['person','type','age'])
from pyspark.sql.functions import *
from pyspark.sql.window import Window
df_a=df.filter(col("type")=='ADULT')
df_c=df.filter(col("type")=='CHILD')

w1=Window.orderBy(desc(col("age")))
w2=Window.orderBy(col("age"))

df_a=df_a.withColumn("id",row_number().over(w1)).withColumnRenamed("person","adult")
df_c=df_c.withColumn("id",row_number().over(w2)).withColumnRenamed("person","child")

df=df_a.join(df_c,'id','left').select("adult","child")
df.show()
---------------------------------------------------------------------------------------------------------------------------------------------
a=spark.createDataFrame([[1,'Ayan'],[1,'Abir']],schema=['id', 'name'])
b=spark.createDataFrame([[1,'Chatterjee'],[0,'DAS']],schema=['id', 'surname'])

a=spark.createDataFrame([[1],[1],[2]],schema=['id'])
b=spark.createDataFrame([[1],[2],[2],[0]],schema=['id'])

from pyspark.sql.types import StringType
from pyspark.sql.functions import lit

c = a.withColumn('id', lit(None).cast(StringType()))
c=c.limit(2)
a=a.union(c)

c=c.limit(1)
b=b.union(c)

Calculate no or rows for each scenario:
Table A INNER JOIN TABLE B = ? 2
Table A LEFT JOIN TABLE B = ?2[1,1][1,1]
Table A RIGHT JOIN TABLE B = ?3
Table A OUTER JOIN TABLE B = ?4 [1,1][1,0][1,1][1,0]
Table A UNION TABLE B = ?2
Table A UNION ALL TABLE B = ?3

a_inner_b=a.join(b,['id'],'inner')
a_inner_b.show()

a_left_b=a.join(b,['id'],'left')
a_left_b.show()

a_right_b=a.join(b,['id'],'right')
a_right_b.show()

a_outer_b=a.crossJoin(b)
a_outer_b.show()

b_anti_a=b.join(a,['id'],'left_anti')
b_anti_a.show()

b_semi_a=b.join(a,['id'],'left_semi')
b_semi_a.show()

a_union_b=a.union(b)
a_union_b.show()

a_unionall_b=a.unionAll(b)
a_unionall_b.show()
-----------------------------------------------------------------------------------------------------
